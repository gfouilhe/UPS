{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5enyRdM5to8"
      },
      "source": [
        " **Représentation vectorielle en sac de mots (Bag of Words)**\n",
        "\n",
        " \n",
        "\n",
        "Nous allons voir comment représenter un document avec des matrices de co-occurence. Nous utilisons pour cela la bibliothèque CounVectorizer de Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2TYL6Z2AH82F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# le corpus, on considère pour simplifier un document par élément de la liste\n",
        "\n",
        "doc_en=[\"Adam is an hyperparameter.\",\"There are some tunas in the beach\",\"A tuna is a fish\",\n",
        "    \"You can tune a piano but you can't tune a fish.\",\n",
        "    \"Fish who eat fish, catch fishes.\",\n",
        "    \"People can tune a fish or a hyperparameter.\",\n",
        "    \"It is hard to catch fish and tune it.\"]\n",
        "\n",
        "vectorizer = CountVectorizer()# tokenization et création du vocabulaire \n",
        "\n",
        "bow=vectorizer.fit_transform(doc_en)# ici on vectorise le document\n",
        "\n",
        "tokens = vectorizer.get_feature_names_out()\n",
        "print(tokens)  #liste des mots du vocabulaire\n",
        "\n",
        "df_bow = pd.DataFrame(data = bow.toarray(),index = ['Doc1','Doc2','Doc3','Doc4','Doc5','Doc6','Doc7'],columns = tokens) \n",
        "\n",
        "print(df_bow) #le dataframe Matrice terme-document"
      ],
      "metadata": {
        "id": "uIrFZTTjOKmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Que remarquez vous ? "
      ],
      "metadata": {
        "id": "0QZUjGR7VKzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "\n",
        "def my_tokenize(doc):\n",
        "    tokens = nlp_en(doc)\n",
        "    return([token.lemma_ for token in tokens])\n",
        "\n",
        "def my_tokenize2(doc):\n",
        "    texte = nlp_en(doc)\n",
        "    tokens = [token.lemma_ for token in texte if not (token.is_stop or token.is_punct)]\n",
        "    return tokens\n",
        "\n",
        "new_vectorizer = CountVectorizer(tokenizer=my_tokenize)  #try with my_tokenize2 and observe the difference\n",
        "new_bow = new_vectorizer.fit_transform(doc_en)\n",
        "tokens = new_vectorizer.get_feature_names_out()\n",
        "print(tokens)  #liste des mots du vocabulaire\n",
        "\n",
        "df_new_bow = pd.DataFrame(data = new_bow.A,index = ['Doc1','Doc2','Doc3','Doc4','Doc5','Doc6','Doc7'],columns=tokens) \n",
        "print(df_new_bow) #le dataframe Matrice terme-document en fréquence\n"
      ],
      "metadata": {
        "id": "lTpmIS9jVR-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La même chose mais maintenant on a une matrice de co-occurence en TF-IDF. Modifier le code suivant pour que les colonnes de la matrice contiennent des lemmes et non pas des termes\n"
      ],
      "metadata": {
        "id": "Pw2iBtVbfpUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english') #observez la différence avant et sans stop words\n",
        "X = tfidf_vectorizer.fit_transform(doc_en) \n",
        "tokens=tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "df_tfidf_bow = pd.DataFrame(np.round(X.A,3),columns=tokens)\n",
        "print(df_tfidf_bow.shape)\n",
        "print(df_tfidf_bow) #le dataframe Matrice terme-document en tf idf\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SwEIt11Jfhkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer maintenant le document suivant en une matrice de co-occurence terme-document. Il vous faut : \n",
        "  * prétraiter le texte\n",
        "    * spliter le texte en phrases et procéder à la tokenization (utilisez la fonction sent_tokenize de NLTK). Voir https://www.nltk.org/api/nltk.tokenize.html\n",
        "\n",
        "    * transformer le texte en minuscule\n",
        "  * puis appliquer les étapes vues précédement pour en extraire la matrice de co-occurence.\n",
        "  * extraire ensuite la matrice de co-occurence des bigrams. Voir le paramètre ngram_range de la fonction CountVectorizer. Voir https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "  "
      ],
      "metadata": {
        "id": "3WQq9zfdj2GI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW5HNLvdun6b",
        "outputId": "d19798be-62c0-4067-8caf-aaf804efe2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_wiki= \"Founded by the Romans, the city was the capital of the Visigothic Kingdom in the 5th century and the capital of the province of Languedoc in the Late Middle Ages and early modern period (provinces were abolished during the French Revolution), making it the unofficial capital of the cultural region of Occitania (Southern France). It is now the capital of the region of Occitania, the second largest region in Metropolitan France. \"\n",
        "# a vous de jouer \n",
        "\n"
      ],
      "metadata": {
        "id": "GNcO-iiTj9p8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}