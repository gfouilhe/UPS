{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1L9JLLQHPZoMRwzYfmKcyM9VME_SHeZrr)"
      ],
      "metadata": {
        "id": "oQvfEkhlqkod"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT3dnp1IkNR0"
      },
      "source": [
        "# TP 1 : pre-processing texts\n",
        "\n",
        "In this practical session, we will see how to pre-process textual data using NLTK and Spacy.\n",
        "\n",
        "Within a computer, text is encoded as a string of characters. \n",
        "In order to analyze textual data within NLP applications, we first need to properly preprocess it. \n",
        "An NLP preprocessing pipeline generally consists of the following steps :\n",
        "* sentence segmentation\n",
        "* tokenisation\n",
        "* normalization: lower-casing, lemmatization, optionally removing stop-words and punctuation \n",
        "* pos-tagging\n",
        "* named entity recognition\n",
        "* parsing\n",
        "\n",
        "The first two steps are necessary, while the others are optional.\n",
        "\n",
        "For these exercises, we will use the modules **NLTK** and **spacy** (already installed on google colab, but some libraries might be missing for your NLTK, we'll see later).\n",
        "\n",
        "NLTK and Spacy both provide ways to carry out tasks such as segmentation, tokenization, lemmatization and pos-tagging.\n",
        "\n",
        "NLTK is a rather old library, but still used a lot. NLTK was built by scholars and researchers as a tool to help you create complex NLP functions.\n",
        "Spacy is more recent one, it implements an NLP pipeline. While NLTK provides access to many algorithms to get something done, spaCy provides the best way to do it (https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/).\n",
        "\n",
        "We will extract information from Wikipedia pages as an example."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0- Upload and read the text files\n",
        "\n",
        "At first, we're going to use a text written in English. Then, we'll try to apply the tools to French.\n",
        "We'll use the wikipedia library to extract pages from wikipedia"
      ],
      "metadata": {
        "id": "NjTuICUXk9oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wikipedia"
      ],
      "metadata": {
        "id": "G-2vyqrvBPXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubFamr89vq_Z"
      },
      "source": [
        "import wikipedia\n",
        "wikipedia.set_lang('en')\n",
        "text_en = wikipedia.page(\"Lovelace\")\n",
        "print(text_en.content[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia.set_lang('fr')\n",
        "text_fr = wikipedia.page(\"Lovelace\")\n",
        "print(text_fr.content[:1000])"
      ],
      "metadata": {
        "id": "zVyConoTxulv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUnGCI8Ad8r3"
      },
      "source": [
        "## 1- Using NLTK\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQynJHRLnzCY"
      },
      "source": [
        "--> **For now, we work on the English file**\n",
        "\n",
        "### 1.1 Sentence Segmentation\n",
        "\n",
        "**Exercise 1:** Breaking the text into Sentences\n",
        "\n",
        "* Import [NLTK](https://www.nltk.org/api/nltk.html)\n",
        "* In NLTK, you can use help(X) to get information about function X works e.g., help(nltk.word_tokenize) to get information about NLTK's word tokenizer.   \n",
        "Use the [help function](https://www.nltk.org/api/nltk.html?highlight=help#module-nltk.help) to see how to use *nltk.sent_tokenize*  \n",
        "* Now use the [sent_tokenize()](https://www.nltk.org/api/nltk.tokenize.html?highlight=sent_tokenize#nltk.tokenize.sent_tokenize) function to segment the text into sentences.\n",
        "* Apply print() to the output of the tokenizer to view the results\n",
        "* How many sentences do we have?\n",
        "\n",
        "You might need to download additional resources for NLTK, e.g. the package 'punkt':\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Lc1gPHAJpQOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfbPFAtenzCZ"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "# get information on nltk.sent_tokenize \n",
        "\n",
        "\n",
        "# Perform sentence segmentation on the English wikipedia page\n",
        "\n",
        "\n",
        "# Print the sentences and the total number of sentences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_PYoZgenzCb"
      },
      "source": [
        "### 1.2 Tokenization\n",
        "\n",
        "**Exercise 2:** Tokenizing a text file \n",
        "\n",
        "* Tokenize the text using NLTK [word_tokenize](https://www.nltk.org/api/nltk.tokenize.html?highlight=word_tokenize)\n",
        "* Inspect the results: does it work well?\n",
        "* How many tokens do we have?\n",
        "* How many words / types / unique tokens do we have? Hint: use numpy.unique(list)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4Jk63dynzCb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Use NLTK to tokenize the text\n",
        "\n",
        "\n",
        "# Print out the tokens, the total number of tokens, and the number of \n",
        "# unique tokens (vocabulary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Pre-processing of French text\n",
        "\n",
        "--> **Now, we will use the French wikipedia page**\n",
        "\n",
        "**Exercise 3** Tokenization for French\n",
        "* Now, try to perform the same pre-processing on the French document\n",
        "* Do you see a problem?\n",
        "* Check the language option for NLTK, does it work better?\n",
        "* Use the *RegexpTokenizer* to solve the issue."
      ],
      "metadata": {
        "id": "IsHXxe4Rl-ZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use NLTK to perform sentence segmentation and tokenization on the \n",
        "# French wikipedia page (same prints)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3Zz8u6kmI2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the language option?\n"
      ],
      "metadata": {
        "id": "UpBob85bhbQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below we define a regex based tokenizer, does it work better?\n",
        "from nltk import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n"
      ],
      "metadata": {
        "id": "iul98ydDhvSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Using Spacy\n",
        "\n",
        "All info about Spacy: https://spacy.io/ ; More info on the pipelines: https://spacy.io/usage/processing-pipelines \n",
        "\n",
        "Spacy is a more realistic library for NLP than NLTK, with higher performances on the basic processing steps. \n",
        "\n",
        "Spacy can be used to directly tokenize any text. \n",
        "With spacy, we build a pipeline that does everything at once. \n",
        "To make it work, you need to **load a model specific to the target language**, for example 'en' for English (there are also some domain specific models).\n",
        "\n",
        "\n",
        "The model corresponds to a processing 'pipeline': \n",
        "  by default, it includes the tokenisation, the lemmatization and the POS tagging\n",
        "\n",
        "Using spacy:\n",
        "- import the spacy module into Python \n",
        "- load all the necessary models, e.g. for English\n"
      ],
      "metadata": {
        "id": "tpak2scybkS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "6UHjeb9KxTEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Then process a text with the pipeline: \n",
        "\n"
      ],
      "metadata": {
        "id": "3FTH0fPnxpOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(content_en)"
      ],
      "metadata": {
        "id": "-Z72vQ3-xWdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Tokenisation\n",
        "\n",
        "**Exercise 4:** Tokenize the text in French\n",
        "* Find a model for French and tokenize the text in the file. Hint: you will need to download the model first, that can be done in the notebook using: *spacy.cli.download( model_name )*\n",
        "* What does contain the *doc* variable? Hint: You can either access Spacy's manual on the internet to find out how to access the information, or look at the built-in help by typing help(doc). https://spacy.io/api/doc\n",
        "* Print the individual tokens. Do you see any error?\n",
        "* How many tokens do we have?\n",
        "* How many words / types / unique tokens do we have (i.e. vocabulary size)?\n",
        "* Use Pandas to better visualize the results"
      ],
      "metadata": {
        "id": "hbLBffVRbwzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dowload a model for French\n"
      ],
      "metadata": {
        "id": "ffGzzZLvdJJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "\n",
        "\n",
        "# Preprocess using spacy's pipeline\n",
        "\n",
        "\n",
        "# Inspect tokens: print out the tokens, the total number of tokens, and the \n",
        "# number of unique tokens (vocabulary) \n",
        "\n"
      ],
      "metadata": {
        "id": "w_QHTVG_ca6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HcJhX3DfCTm"
      },
      "source": [
        "#### Pandas\n",
        "\n",
        "You can use Pandas to better visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VW_9Glf1Gsz"
      },
      "source": [
        "# Display a pandas dataframe with the tokens\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98dC9lMGoZIc"
      },
      "source": [
        "### 2.2 Sentence segmentation\n",
        "\n",
        "**Exercise 5:**\n",
        "Apart from token segmentation, Spacy has also automatically segmented our document intro sentences. \n",
        "* Print out the different sentences of the document.\n",
        "Hint: Look at the \"Data descriptors \" in the help page for 'doc'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t"
      ],
      "metadata": {
        "id": "G60bS7aW0PRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQmuXqxuoyVC"
      },
      "source": [
        "# Print the sentences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVFRCsbznzCd"
      },
      "source": [
        "## 3- Further pre-processing \n",
        "\n",
        "We saw earlier that the most frequent words are punctuation and function words. \n",
        "In order to find the most important words, e.g. to index documents, we probably want to remove these tokens.\n",
        "We are thus now going to **remove punctuation signs and \"stop words\"**.\n",
        "Note that for a full normalization, we would probably also lower case the first word of each sentence, and all words that are not tagged as proper nouns (but it requires pos tagging).\n",
        "\n",
        "Exercise 6:\n",
        "* Define a function that segments, tokenizes, removes punctuation and removes stop words. \n",
        "  * **Hint** look at *string.punctuation*\n",
        "  * **Hint** *spacy.lang* contains language specific data for each language, in particular stop words lists.\n",
        "* Apply this function to the french wikipedia page\n",
        "* Print the total number of unique tokens after this pre-processing and the first 100 tokens on the cleaned version of the text\n",
        "* Display a panda dataframe containing an ordered list of the tokens after cleaning and their frequency\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that cleans a text by segmenting into sentences, \n",
        "# tokenizing, removing punctuation and stop words\n",
        "\n",
        "\n",
        "\n",
        "# Apply the function to the French wikipedia page\n",
        "\n",
        "\n",
        "# Print the number of unique tokens ater cleaning\n",
        "\n",
        "\n",
        "# Print the first 100 tokens of the cleaned version of the text\n"
      ],
      "metadata": {
        "id": "rL7xOhzj5M5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a panda dataframe containing an ordered list of the tokens after \n",
        "# cleaning and their frequency\n"
      ],
      "metadata": {
        "id": "G98n1c9U01K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBihujqWvc0T"
      },
      "source": [
        "## 4- POS tagging\n",
        "\n",
        "Remember that the model corresponds to a processing 'pipeline' in Spacy: \n",
        "  - by default, it includes the tokenisation, the lemmatization and the POS tagging\n",
        "\n",
        "**Exercise 7**\n",
        "- print each individual token, together with its lemmatized form and part of speech tag\n",
        "- Use Panda to better visualize the results\n",
        "- Look at the results, do you see any error?\n",
        "- You can use the method 'spacy.explain' to have information about some annotation, for example the POS tags. Apply it to each POS tag to get a more detailed label.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q1L4ApskpmF"
      },
      "source": [
        "# Print tokens, lemmas, and pos tags\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebtSSJ28vc0V"
      },
      "source": [
        "#### Pandas\n",
        "\n",
        "You can use Pandas to better visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nmgrYgavc0W"
      },
      "source": [
        "# Display a panda dataframe containing the tokens and associated lemmas and POS \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXStTWgunYqT"
      },
      "source": [
        "# Use the method 'explain' to get a more detailed version of the POS tags\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-1IyOOTpkML"
      },
      "source": [
        "## 5- Named entity recognition\n",
        "\n",
        "As part of the preprocessing pipeline, Spacy has also carried out named entity recognition.\n",
        "\n",
        "**Exercise 8:**\n",
        "* print out each named entity, together with the label assigned to it\n",
        "* what do the labels stand for?\n",
        "* Use the module called 'displacy' to visualize the Named Entities directly in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOu1lpzop19t"
      },
      "source": [
        "# print out each named entity, together with the label assigned to it\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the method 'explain' to get a more detailed version of the NE tags\n"
      ],
      "metadata": {
        "id": "cDvqYQAmuFEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the module called 'displacy' to visualize the Named Entities directly in the text\n"
      ],
      "metadata": {
        "id": "TLcBxoP-2HAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mp3IrdNnzCk"
      },
      "source": [
        "## 6- Parsing \n",
        "\n",
        "Finally, as part of the pipeline, Spacy has also performed a dependency parsing (note that each module can de disabled if not needed).\n",
        "\n",
        "**Exercise 9:** \n",
        "\n",
        "* Retrieve the information from the dependency parses: dependent and head of each token for the first sentence of the document\n",
        "* Use displacy to visualize a parse tree: first try with a simple sentence (e.g. *La petite brise la glace.*) then use the first sentence of the document.\n",
        "* Navigating the parse tree. Each element of the tree is associated to attributes: you can use them to inspect the different elements of the trees: \n",
        "  * Define a Panda dataframe with each token id associated to its head, with the relation between them. The eventual children of the current token are also printed.\n",
        "* Print all the adjectives and the noun they modify\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjl3ecSLnzCl"
      },
      "outputs": [],
      "source": [
        "# Retrieve the information from the dependency parses: dependent and head of \n",
        "# each token for the first sentence of the document\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use displacy to visualize a parse tree: \n",
        "# first try with a simple sentence (e.g. *La petite brise la glace.*) \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PhgHsBGA4wkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then use the first sentence of the wikipedia document\n"
      ],
      "metadata": {
        "id": "g9h01UxB4l5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a panda dataframe containing all the information about a token:\n",
        "# text, pos, dep relation to head, head text, head pos, children \n"
      ],
      "metadata": {
        "id": "sFKm4O8J4vzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg69R5ncnzCl"
      },
      "outputs": [],
      "source": [
        "# Extract all adjectives and the noun they modify\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yg5PsJX953p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7- Putting it all together\n",
        "\n",
        "Now we are going to use the skills practiced in the preceding exercises to build a simple question-answering system on a toy dataset (in French).\n",
        "\n",
        "We will focus on specific questions of the form \"Qui a peint X ?\". \n",
        "We will define patterns based on differents ways of formulating this question, and use them to extract the answer from a small toy corpus based on wikipedia pages on paintings. \n",
        "\n",
        "When you're done with this exercise, try to answer other types of questions, such as \"Où est exposé X ?\", \"Quand a été peinte X ?\".\n",
        "\n",
        "Below, we reload the spacy French model adding specific options to merge named entities containing multiple tokens."
      ],
      "metadata": {
        "id": "MXsrDptD6xv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "\n",
        "nlp.add_pipe(\"merge_entities\")\n",
        "nlp.add_pipe(\"merge_noun_chunks\")"
      ],
      "metadata": {
        "id": "TOXL6NSP58sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the list of questions we will consider. \n",
        "You also need a corpus of source documents, you can find it on Moodle (corpus_qa.txt)."
      ],
      "metadata": {
        "id": "P9k6jkG1D6LD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_list = [\n",
        "    'La Joconde est un tableau de qui ?',\n",
        "    'Le radeau de la méduse est une peinture réalisée par qui ?'\n",
        "]\n",
        "corpus = 'corpus_qa.txt'"
      ],
      "metadata": {
        "id": "qVi5ElJD5sgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 10:** In this part, we focus on the first question. This question is designed to be similar to the document containing the answer. We can thus define a pattern based on its structure to extract the answer from the document.\n",
        "\n",
        "- Process the question using the spacy nlp pipeline \n",
        "- display its parse tree and / or print a Pandas dataframe containing information from the parse tree\n",
        "- Now to define a lexico-syntactic pattern, you are going to use spacy *DependencyMatcher* : https://spacy.io/usage/rule-based-matching#dependencymatcher\n",
        "  * Look at the doc to understand how it works\n",
        "  * Define a pattern that should match 'qui ' in the question "
      ],
      "metadata": {
        "id": "OWgaL0ax6JLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the parse tree of the first question \n"
      ],
      "metadata": {
        "id": "aVqVbiGR6qI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import DependencyMatcher\n",
        "# Define a lexico-syntactic pattern that allows to retrieve the answer\n",
        "\n",
        "pattern = [\n",
        "    #...\n",
        "\n",
        "]\n",
        "\n",
        "# If you match the pattern to the original question, it should output 'qui'\n"
      ],
      "metadata": {
        "id": "REe4B8iK8Cow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 11**: Retrieve matching documents \n",
        "Retrieve the documents that are relevant to the question, i.e. the ones containing the keyword 'La Joconde'.\n",
        "\n",
        "It is recommended to define a function, that could be used for the next exercises. It could work by:\n",
        "\n",
        "* first indexing all the documents using the named entities present in the document (i.e. build a dictionnary mapping a named entity to all documents where it is present)\n",
        "* now the *retrieve_documents(...)* method should try to match the input keywork with a named entity and return the matching documents.\n",
        "* Test the function with 'Joconde':\n",
        "  * Does it work with the method based on named entities?\n",
        "  * Add a backup solution : if no document is found,simply try to find the string corresponding to the keyword in the document\n"
      ],
      "metadata": {
        "id": "HZqL5iJw_19P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the document that is relevant to the question, i.e. the one \n",
        "# containing 'La Joconde'\n"
      ],
      "metadata": {
        "id": "CIIGDI23ARAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 12:** Test the pattern\n",
        "\n",
        "- Apply the pattern to each sentence of the retrieved document, do you find the right answer?"
      ],
      "metadata": {
        "id": "NUpZ5GisTtd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Apply the pattern to each sentence of this document, do you find the right answer?\n"
      ],
      "metadata": {
        "id": "vtYnSrMz6qLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 13:** Now define a pattern to answer the second question, and find the answer!\n",
        "\n",
        "Be careful, there is a little issue here:\n",
        "- Display the parse tree for the question and the matching document: what do you observe?\n",
        "- Build a pattern to match the question and the answer (Hint : you can use a list of dependency relations in the pattern, using e.g. *\"RIGHT_ATTRS\": {\"DEP\": {\"IN\":[ \"acl\", \"advcl\" ] }*\n",
        "- Finally, retrieve the answer"
      ],
      "metadata": {
        "id": "0cE7cdLXCtoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the parse tree of the second question \n"
      ],
      "metadata": {
        "id": "c0wA68Pp6qN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the parse tree of the matching document (first and unique sentence)\n"
      ],
      "metadata": {
        "id": "9_jtWfWoU-ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a pattern that matches both the question and answer\n"
      ],
      "metadata": {
        "id": "IMZgJQUUVThn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the answer\n"
      ],
      "metadata": {
        "id": "tldfogSsiUc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 14:** Use the patterns defined to find the answers to the questions below. Here, we know that we're looking for the painter, we don'y want to match the question to the answer, we want to test known patterns to find the right answer.\n",
        "\n",
        "- find a way to extract the name of the painting from the question\n",
        "- retrieve the relevant document\n",
        "- test the patterns defined previously to extract the correct answer"
      ],
      "metadata": {
        "id": "51yXt9EelmBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_questions = [\n",
        "    'Qui a peint American Gothic ?',\n",
        "    'Qui est l\\'auteur de la peinture La Nuit étoilée',\n",
        "    'Qui a réalisé Un Garrochista ?',\n",
        "]"
      ],
      "metadata": {
        "id": "95jnMLHXlpNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IaVt7W9dmAnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 15:** To go further\n",
        "\n",
        "Try to:\n",
        "- find who painted 'Le Cri' and 'Arearea\n",
        "- where are located 'La Joconde' and 'Arearea'"
      ],
      "metadata": {
        "id": "UBHY74xPmBIc"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}