{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " **Représentation vectorielle denses (les plongements de mots)**\n",
        "\n",
        " \n",
        "\n",
        "Nous allons voir comment représenter un document avec des embeddings pre-entrainés. Nous utilisons pour cela les représentations word2Vec telles qu'implémenter dans Spacy puis dans la librairie Gensim.\n"
      ],
      "metadata": {
        "id": "x67vxX7IyCFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import re\n",
        "from gensim.models import word2vec\n",
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "hMWcHtv10j16"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.downloader\n",
        " \n",
        "for model_name in list(gensim.downloader.info()['models'].keys()):\n",
        "  print(model_name)\n",
        "\n",
        "google_news_vectors = gensim.downloader.load('word2vec-google-news-300')   #3 million 300-dimension word vectors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srO4dLL8TK1S",
        "outputId": "7461e017-38d8-4f35-c652-84390ab88a65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fasttext-wiki-news-subwords-300\n",
            "conceptnet-numberbatch-17-06-300\n",
            "word2vec-ruscorpora-300\n",
            "word2vec-google-news-300\n",
            "glove-wiki-gigaword-50\n",
            "glove-wiki-gigaword-100\n",
            "glove-wiki-gigaword-200\n",
            "glove-wiki-gigaword-300\n",
            "glove-twitter-25\n",
            "glove-twitter-50\n",
            "glove-twitter-100\n",
            "glove-twitter-200\n",
            "__testing_word2vec-matrix-synopsis\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google_news_vectors['paris']"
      ],
      "metadata": {
        "id": "BtiRT4VR6PGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Les similarités**"
      ],
      "metadata": {
        "id": "_t-EjTpsDwGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capital = google_news_vectors.most_similar([\"Paris\", \"Britain\"], [\"France\"])#, topn=1)\n",
        "print(capital)"
      ],
      "metadata": {
        "id": "lBmRnGXNcO5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_words = google_news_vectors.most_similar('intelligence')\n",
        "print (sim_words)"
      ],
      "metadata": {
        "id": "xqzWHRCD7Wro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar=google_news_vectors.most_similar(positive=['girl'])\n",
        "\n",
        "less_similar=google_news_vectors.most_similar(negative=['girl'])\n",
        "\n",
        "print(most_similar)\n",
        "print(less_similar)"
      ],
      "metadata": {
        "id": "RtqhnEFjC5j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Affichage des vecteurs en 2D grâce à TSNE**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "muQNr87X9y-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def affichage_similarite_2D(model, word):\n",
        "  arr = np.empty((0,300), dtype='f')\n",
        "  word_labels = [word]\n",
        "  \n",
        "  # get close words\n",
        "  close_words = model.wv.similar_by_word(word)\n",
        "  \n",
        "  # add the vector for each of the closest words to the array\n",
        "  arr = np.append(arr, np.array([model[word]]), axis=0)\n",
        "  \n",
        "  for wrd_score in close_words:\n",
        "    wrd_vector = model[wrd_score[0]]\n",
        "    word_labels.append(wrd_score[0])\n",
        "    arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
        "  \n",
        "  # find tsne coords for 2 dimensions\n",
        "  \n",
        "  tsne = TSNE(n_components=2, random_state=0)\n",
        "  np.set_printoptions(suppress=True)\n",
        "  Y = tsne.fit_transform(arr)\n",
        "  \n",
        "  x_coords = Y[:, 0]\n",
        "  y_coords = Y[:, 1]\n",
        "  \n",
        "  # display scatter plot\n",
        "  plt.scatter(x_coords, y_coords)\n",
        "  \n",
        "  for label, x, y in zip(word_labels, x_coords, y_coords):\n",
        "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
        "  plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
        "  plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
        "  plt.show()\n",
        "\n",
        " \n",
        "\n",
        "affichage_similarite_2D(google_news_vectors, 'girl')\n",
        "\n"
      ],
      "metadata": {
        "id": "qR-6VKc-8-uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Les analogies (cf. slides 55 à 59)**"
      ],
      "metadata": {
        "id": "qkqXNOKiDpHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pp(obj):\n",
        "    print(pd.DataFrame(obj))\n",
        "    \n",
        "def analogy(worda, wordb, wordc):\n",
        "    result = google_news_vectors.most_similar(negative=[worda], \n",
        "                                positive=[wordb, wordc])\n",
        "    return result[0][0]\n",
        "\n",
        "countries = ['australia', 'canada', 'germany', 'ireland', 'italy']\n",
        "\n",
        "#ce qui revient à \"us - country + hamburger, i.e. us is to hamburger as sim(hamburger) is to country\"\n",
        "\n",
        "foods = [analogy('us', 'hamburger', country) for country in countries]\n",
        "pp(zip(countries,foods))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-lBd9PswDsrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#on plot\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "def plot_data(orig_data, labels):\n",
        "    pca = PCA(n_components=2)\n",
        "    data = pca.fit_transform(orig_data)\n",
        "    plt.figure(figsize=(7, 5), dpi=100)\n",
        "    plt.plot(data[:,0], data[:,1], '.')\n",
        "    for i in range(len(data)):\n",
        "        plt.annotate(labels[i], xy = data[i])\n",
        "    for i in range(len(data)//2):\n",
        "        plt.annotate(\"\",\n",
        "                xy=data[i],\n",
        "                xytext=data[i+len(data)//2],\n",
        "                arrowprops=dict(arrowstyle=\"->\",\n",
        "                                connectionstyle=\"arc3\")\n",
        "        )\n",
        "       \n",
        "labels = countries + foods\n",
        "data = [google_news_vectors[w] for w in labels]\n",
        "plot_data(data, labels)"
      ],
      "metadata": {
        "id": "UfEIeQxBEMyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#try other biased analogies....\n",
        "#try with the analogies of the form a is to b as c is to d\n"
      ],
      "metadata": {
        "id": "kNdrObWmGiA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Création de vecteurs à partir d'un corpus de textes**"
      ],
      "metadata": {
        "id": "Cz5zetY5Bf3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import re\n",
        "\n",
        "\n",
        "train = fetch_20newsgroups(subset='train')\n",
        "\n"
      ],
      "metadata": {
        "id": "hCo3_TTxApBU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "    \"\"\"tokenization, ne garder que les lettres, suppression des headers, etc. \"\"\"\n",
        "    lines = re.split('[?!.:]\\s', re.sub('^.*Lines: \\d+', '', re.sub('\\n', ' ', text)))\n",
        "    return [re.sub('[^a-zA-Z]', ' ', line).lower().split() for line in lines]\n",
        "\n",
        "sentences = [line for text in train.data for line in clean(text)]\n",
        "\n",
        "print('raw data:\\n\\n', train.data[0])\n",
        "print('example input:\\n', sentences[0])\n",
        "\n",
        "model = Word2Vec(sentences, workers=4, size=300, min_count=50, window=10, sample=1e-3)  # voir la documentation de Word2Vec\n",
        "\n",
        "affichage_similarite_2D(model, 'sport')\n",
        "\n"
      ],
      "metadata": {
        "id": "XNs5x7qQ8VwD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}