{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQYDsCOaiRvI"
      },
      "outputs": [],
      "source": [
        "!wget https://www.irit.fr/~Thomas.Pellegrini/ens/M2ML2/TP_PS/m2_ml2_tp_perceptron_structure.zip\n",
        "!unzip m2_ml2_tp_perceptron_structure.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8HDkD7Ck5Sa"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = 'data'\n",
        "\n",
        "import pos_corpus as pcc\n",
        "import id_feature as idfc\n",
        "\n",
        "import discriminative_sequence_classifier as dsc\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiPUXJCZzWCD"
      },
      "source": [
        "Sujet TP2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDb_xyLgzYJl"
      },
      "source": [
        "# Perceptron structuré pour du POS tagging\n",
        "\n",
        "Le POS tagging est une tâche de classification dite structurée. Il s'agit de prédire les tags syntaxiques (POS tags) des mots d'une phrase.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNoITxohzGFu"
      },
      "source": [
        "# Chargement du dataset CoNLL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL0KT6yaM3m_"
      },
      "outputs": [],
      "source": [
        "corpus = pcc.PostagCorpus()\n",
        "train_seq = corpus.read_sequence_list_conll(DATA_PATH + \"/train-02-21.conll\",\n",
        "                                            max_sent_len=10, max_nr_sent=1000)\n",
        "\n",
        "test_seq = corpus.read_sequence_list_conll(DATA_PATH + \"/test-23.conll\",\n",
        "                                           max_sent_len=10, max_nr_sent=1000)\n",
        "\n",
        "# nous n'utlisons pas le dev ici\n",
        "# dev_seq = corpus.read_sequence_list_conll(DATA_PATH + \"/dev-22.conll\",\n",
        "#                                           max_sent_len=10, max_nr_sent=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDJBxBBnpSIn"
      },
      "source": [
        "Afficher des exemples de séquences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cFfkPQnO0J_"
      },
      "outputs": [],
      "source": [
        "# on regarde des exemples de phrases\n",
        "seq_ind = 3\n",
        "seq = train_seq[seq_ind]\n",
        "\n",
        "print(seq)\n",
        "print('x', seq.x)\n",
        "print('y', seq.y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtUOwadc0eWN"
      },
      "source": [
        "# Les *feature functions* ou *potentials* \n",
        "\n",
        "Nous allons dans un premier temps utiliser des feature functions qui mimiquent les HMM : \n",
        "\n",
        "*   *Initial features* : features en position 0 dans les phrases qui encodent l'identité des tags en position 0, exemple : init_tag:pron\n",
        "*   *Emission features* : features mot/tag, appelés \"nodes\" (émission), exemple : id:many::adj\n",
        "*   *Transition features* : features tag/tag, appelés \"edges\" (transition), exemple : prev_tag:adv::num\n",
        "*   *Final features* : features en position finale dans les phrases: l'identité des tags en position finale, exemple : final_prev_tag:num\n",
        "\n",
        "\n",
        "Dans le fichier ```id_feature.py```, est définie une classe ```IDFeatures``` qui crée ces features à partir des séquences mot/tag du train : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXpykVjPOXEp"
      },
      "outputs": [],
      "source": [
        "## Instancier et créer les feature functions sur le train \n",
        "feature_mapper = idfc.IDFeatures(train_seq)\n",
        "feature_mapper.build_features()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccA-iSHhof6i"
      },
      "source": [
        "Afficher tous les noms de feature functions et les compter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF7uH7jCOmH9"
      },
      "outputs": [],
      "source": [
        "nb = 0\n",
        "for el in feature_mapper.??.??:\n",
        "    print(el)\n",
        "    nb += 1\n",
        "\n",
        "print(nb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzDeyf_PrJYa"
      },
      "source": [
        "Afficher tous les noms de feature functions de la séquence d'indice ```seq_ind```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nORyOtbiOrn-"
      },
      "outputs": [],
      "source": [
        "current_feature_list = feature_mapper.feature_list[??]\n",
        "\n",
        "??\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCJ6oWK_Pqck"
      },
      "source": [
        "# Le perceptron structuré (Collins, 2002)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjSt8URphvH_"
      },
      "source": [
        "Le perceptron struturé est un classifieur de séquences à séquences, dit discriminant. \n",
        "\n",
        "Ce type d'approche modélise la probabilité conditionnelle d'une séquence de tags $\\boldsymbol y$ étant donnée une séquence de mots $\\boldsymbol x$, à l'aide de produits scalaires entre le vecteur de poids $\\boldsymbol w$ (à apprendre) et les *feature functions*, dont des exemples ont été donnés ci-dessus :  \n",
        "\n",
        "\\begin{equation} \n",
        "P(\\boldsymbol y | \\boldsymbol x ; \\boldsymbol w) = \\displaystyle\\frac{1}{Z(\\boldsymbol w, \\boldsymbol x)}\\exp \\Big( \\boldsymbol w \\cdot \\boldsymbol f_{\\text{init}}(\\boldsymbol x, y_0)+\\sum_{i=0}^{N-2}\\boldsymbol w \\cdot \\boldsymbol f_{\\text{trans}}(i, \\boldsymbol x, y_i, y_{i+1}) +\\boldsymbol w  \\cdot \\boldsymbol f_{\\text{final}}(\\boldsymbol x, y_{N-1}) + \\sum_{i=0}^{N-1}\\boldsymbol w \\cdot \\boldsymbol f_{\\text{emission}}(i, \\boldsymbol x, y_i)\\Big) \n",
        "\\end{equation} \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCtDfDd5OAau"
      },
      "source": [
        "Nous vous fournissons un squelette du code du perceptron structuré dans une cellule ci-dessous, à compléter.\n",
        "\n",
        "\n",
        "La classe ```StructuredPerceptron``` hérite de la classe ```DiscriminativeSequenceClassifier```, qui elle-même hérite de la classe ```SequenceClassifier```. \n",
        "\n",
        "Ces deux classes sont données dans les fichiers respectifs ```discriminative_sequence_classifier.py``` et ```sequence_classifier.py```. \n",
        "\n",
        "Prenez le temps de lire ce que contiennent ces fichiers.\n",
        "\n",
        "Voici le diagramme UML qui résume ces dépendances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhulelWRPG81"
      },
      "source": [
        "<img src=\"https://www.irit.fr/~Thomas.Pellegrini/ens/M2ML2/TP_PS/diagramme_UML_structured_perceptron.png\"\n",
        "     alt=\"Digramme UML\"\n",
        "     style=\"float: left; margin-right: 1px;\"\n",
        "     />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIPSfGQ-AC1U"
      },
      "source": [
        "## L'algorithme d'apprentissage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGt6rxY9AFdb"
      },
      "source": [
        "Initialiser les poids du perceptron avec le vecteur nul : $\\boldsymbol w = \\boldsymbol 0$\n",
        "\n",
        "Pour $i=1\\ldots T$\n",
        "\n",
        "*   Pour chaque exemple d'apprentissage ($\\boldsymbol x, \\boldsymbol y$)\n",
        "\n",
        "    1.    Générer une séquence de prédictions : $\\boldsymbol z = argmax_{\\boldsymbol z} \\boldsymbol w \\cdot \n",
        "\\boldsymbol f (\\boldsymbol x, \\boldsymbol y)$\n",
        "                 \n",
        "    2.    Pour chaque Si $\\boldsymbol z \\neq \\boldsymbol y$, faire : \n",
        "                 \n",
        "\\begin{equation*}\n",
        "                    \\boldsymbol w \\leftarrow \\boldsymbol w + \\boldsymbol f (\\boldsymbol x, \\boldsymbol y) - \\boldsymbol f (\\boldsymbol x, \\boldsymbol z)\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "\n",
        "La fonction $\\boldsymbol f$ correspond aux feature functions extraits pour les séquences $\\boldsymbol x, \\boldsymbol z$, et sont accessibles à l'aide de l'objet ```feature_mapper```, vu ci-dessus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ULNJrc3DMp-"
      },
      "source": [
        "## Travail à faire\n",
        "\n",
        "\n",
        "Vous devez coder la méthode ```perceptron_update()``` de la classe ```StructuredPerceptron``` de la cellule ci-dessous.\n",
        "\n",
        "\n",
        "Cette fonction prend en entrée ***une séquence*** et effectue les lignes 1 et 2 de l'algorithme sur cette séquence. \n",
        "\n",
        "Elle retourne ```num_labels, num_mistakes``` qui sont respectivement le nombre d'éléments de la séquence à traiter et le nombre d'erreurs commises par le modèle sur la séquence.\n",
        "\n",
        "Détaillons ces deux lignes : \n",
        "\n",
        "1.   Pour générer la séquence de prédictions $\\boldsymbol z$, faire un décodage Viterbi sur la séquence.\n",
        "\n",
        "2.   Le vecteur de poids $\\boldsymbol w$ du perceptron correspond à ```self.parameters```.\n",
        "\n",
        "La mise à jour du vecteur est faite en testant chaque élément de la séquence prédite $z_i$ avec $i=0\\ldots L-1$, avec $L$ la longueur de la séquence. Les features étant tous binaires, si une prédiction est fausse pour une position $i$, alors il faut ajouter ou retrancher 1 aux quatre types de feature functions. \n",
        "\n",
        "Plus précisément : \n",
        "\n",
        "*   Pour la première position dans la séquence ($i=0$) :\n",
        "    *    si $z_0 \\neq y_0$, faire : \n",
        "\n",
        "    \\begin{eqnarray}\n",
        "      \\boldsymbol w[\\text{initial features}((\\boldsymbol x, \\boldsymbol y_0))] & \\mathrel{+}=&  1 \\\\\n",
        "      \\boldsymbol w[\\text{initial features}((\\boldsymbol x, \\boldsymbol z_0))] & \\mathrel{-}=&  1\n",
        "    \\end{eqnarray}\n",
        "    \n",
        "* Puis pour $i=0\\ldots L-1$ : \n",
        "    *  si $z_i \\neq y_i$, modifier $ \\boldsymbol w$ de la même façon mais en considérant les *emission features*  et les *transition features*. Attention, les *transition features* ne sont pertinents que pour $i>0$.\n",
        "* Enfin pour la dernière position $i=L-1$, il faut aussi considérer les *final features*.\n",
        "\n",
        "\n",
        "***Aide***\n",
        "\n",
        "Pour récupérer les quatre types de feature functions, ```feature_mapper``` a les méthodes suivantes :  \n",
        "\n",
        "*    ```get_initial_features(sequence, y)``` \n",
        "*    ```get_emission_features(sequence, i, y)``` \n",
        "*    ```get_transition_features(sequence, i, y, y_prev)``` \n",
        "*    ```get_final_features(sequence, y_prev)``` \n",
        "\n",
        "Avec $i$ la position dans une séquence, ```y``` le tag à la position ```i``` de la vérité terrian ou bien issu de la prédiction, et ```y_prev``` le tag à la position ```i-1```, lorsqu'elle existe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-6Jc_o6-_Ad"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-R1v1wgO-b-"
      },
      "outputs": [],
      "source": [
        "class StructuredPerceptron(dsc.DiscriminativeSequenceClassifier):\n",
        "    \"\"\" Implements Structured Perceptron\"\"\"\n",
        "\n",
        "    def __init__(self, observation_labels, state_labels, feature_mapper,\n",
        "                 num_epochs=10, learning_rate=1.0, averaged=True):\n",
        "      \n",
        "        dsc.DiscriminativeSequenceClassifier.__init__(self, observation_labels, state_labels, feature_mapper)\n",
        "        self.num_epochs = num_epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.averaged = averaged\n",
        "        self.params_per_epoch = []\n",
        "\n",
        "    def train_supervised(self, dataset):\n",
        "        self.parameters = np.zeros(self.feature_mapper.get_num_features())\n",
        "        num_examples = dataset.size()\n",
        "        for epoch in range(self.num_epochs):\n",
        "            num_labels_total = 0\n",
        "            num_mistakes_total = 0\n",
        "            for i in range(num_examples):\n",
        "                sequence = dataset.seq_list[i]\n",
        "                num_labels, num_mistakes = self.perceptron_update(sequence)\n",
        "                num_labels_total += num_labels\n",
        "                num_mistakes_total += num_mistakes\n",
        "            self.params_per_epoch.append(self.parameters.copy())\n",
        "            acc = 1.0 - num_mistakes_total / num_labels_total\n",
        "            print(\"Epoch: %i Accuracy: %f\" % (epoch, acc))\n",
        "        self.trained = True\n",
        "\n",
        "        if self.averaged:\n",
        "            new_w = 0\n",
        "            for old_w in self.params_per_epoch:\n",
        "                new_w += old_w\n",
        "            new_w /= len(self.params_per_epoch)\n",
        "            self.parameters = new_w\n",
        "\n",
        "    def perceptron_update(self, sequence):\n",
        "    \n",
        "        # ----------\n",
        "        # Exercice \n",
        "        num_labels, num_mistakes = 0, 0\n",
        "        \n",
        "        ?? \n",
        "\n",
        "        return num_labels, num_mistakes\n",
        "        #\n",
        "        # ----------\n",
        "\n",
        "    def save_model(self, dir):\n",
        "        fn = open(dir + \"parameters.txt\", 'w')\n",
        "        for p_id, p in enumerate(self.parameters):\n",
        "            fn.write(\"%i\\t%f\\n\" % (p_id, p))\n",
        "        fn.close()\n",
        "\n",
        "    def load_model(self, dir):\n",
        "        fn = open(dir + \"parameters.txt\", 'r')\n",
        "        for line in fn:\n",
        "            toks = line.strip().split(\"\\t\")\n",
        "            p_id = int(toks[0])\n",
        "            p = float(toks[1])\n",
        "            self.parameters[p_id] = p\n",
        "        fn.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4uPAFixWE6Z"
      },
      "source": [
        "Instancier le perceptron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpOq8OAaRHsI"
      },
      "outputs": [],
      "source": [
        "sp = StructuredPerceptron(corpus.word_dict,\n",
        "                          corpus.tag_dict,\n",
        "                          feature_mapper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLd7tMJTWJ_T"
      },
      "source": [
        "Réaliser un entraînement sur 10 epochs : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-rUzmKBROvn"
      },
      "outputs": [],
      "source": [
        "??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr5BgO2LWUOY"
      },
      "source": [
        "Réaliser un décodage Viterbi et une évaluation sur le corpus de test. Afficher l'accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L49sWWnqRZMA"
      },
      "outputs": [],
      "source": [
        "??\n",
        "\n",
        "print(\"Décodage Viterbi subset Test, acc=%.2f%%\"%(100.*acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDogf3TgWf8R"
      },
      "source": [
        "Réaliser un décodage Posterior et une évaluation sur le corpus de test. Afficher l'accuracy. Est-ce meilleur que Viterbi ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWuflYirWmMQ"
      },
      "outputs": [],
      "source": [
        "??\n",
        "\n",
        "print(\"Décodage Posterior subset Test, acc=%.2f%%\"%(100.*acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otgwX2IDXlD0"
      },
      "source": [
        "Y-a-t-il du sur-apprentissage ? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTZsSfKSeyKl"
      },
      "outputs": [],
      "source": [
        "??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stgfUcm1Xpkr"
      },
      "source": [
        "# Jeu de *feature functions* étendu\n",
        "\n",
        "\n",
        "Tester à nouveau le perceptron mais cette fois avec les feature functions étendues mises à disposition dans le fichier ```extended_feature.py```.\n",
        "\n",
        "\n",
        "Quels types de feature functions sont ajoutées par rapport à la classe précédente ```IDFeatures``` ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivbFfPcvZGTG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdFE-pM0et9I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "d776b604cc3904292aea37f4daa2a46de34e87e5ec4d2ed68e9e515ab9feb7f7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
